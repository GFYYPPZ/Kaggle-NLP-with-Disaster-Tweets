{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s0DBGcZfs0FJ"
   },
   "outputs": [],
   "source": [
    "# e.g. if using google colab import drive, uncomment lines below\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LX0ia6JVtFjr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as sk_OLS\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD0dQabauB7z"
   },
   "source": [
    "# Part (a): Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uRBDrBxYtBbk"
   },
   "outputs": [],
   "source": [
    "#====================================================#\n",
    "# YOUR CODE HERE:\n",
    "#   Import train and test csv files.\n",
    "#   You should use the pd.read_csv function.\n",
    "#   You should set the index_col parameter to equal 'id'.\n",
    "#====================================================#\n",
    "\n",
    "train_data = pd.read_csv('train.csv', index_col = 'id')\n",
    "test_data  = pd.read_csv('test.csv', index_col = 'id')\n",
    "\n",
    "#====================================================#\n",
    "# END YOUR CODE\n",
    "#====================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword location                                               text  \\\n",
       "id                                                                          \n",
       "1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5         NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...       ...      ...                                                ...   \n",
       "10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "10872     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "       target  \n",
       "id             \n",
       "1           1  \n",
       "4           1  \n",
       "5           1  \n",
       "6           1  \n",
       "7           1  \n",
       "...       ...  \n",
       "10869       1  \n",
       "10870       1  \n",
       "10871       1  \n",
       "10872       1  \n",
       "10873       1  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dG7kzzuDvlBr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (7613, 3)\n",
      "Test Data Shape: (3263, 3)\n",
      "Number of labels = 1 in train dataset as percentage: 42.97%\n",
      "Number of labels = 0 in train dataset as percentage: 57.03%\n"
     ]
    }
   ],
   "source": [
    "#====================================================#\n",
    "# YOUR CODE HERE:\n",
    "#   Get the index values for X_train and y_train.\n",
    "#   Get the data values for X_train and y_train.\n",
    "#   Get the index values for X_test.\n",
    "#   Get the index values for y_test.\n",
    "#====================================================#\n",
    "\n",
    "# get train indices\n",
    "X_train_id = train_data.index\n",
    "y_train_id = train_data.index\n",
    "# get train data\n",
    "X_train    = train_data.drop(labels = 'target', axis = 1)\n",
    "y_train    = train_data.target\n",
    "\n",
    "# get test indices\n",
    "X_test_id  = test_data.index\n",
    "# get test data\n",
    "X_test     = test_data\n",
    "\n",
    "#====================================================#\n",
    "# END YOUR CODE\n",
    "#====================================================#\n",
    "\n",
    "print(f\"Train Data Shape: {X_train.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"Number of labels = 1 in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 0 in train dataset as percentage: {((y_train == 0).sum() / (X_train.shape[0])) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_16NkNGt3Kx"
   },
   "source": [
    "### Part (a), Question 1: How many training and test data points are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uqBPKEYt7W5"
   },
   "source": [
    "### Answer:\n",
    "> + In the training set, there are 7613 observations\n",
    "> + In the testing set, there are 3263 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEVclzmqt9TF"
   },
   "source": [
    "### Part (a), Question 2: what percentage of the training tweets are of real disasters, and what percentage is not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J7911HiucTb"
   },
   "source": [
    "### Answer:\n",
    "> + 42.97% of the training tweets contain real disasters\n",
    "> + 57.03% of the training tweets do not contain real disasters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrzjUIXfuHEt"
   },
   "source": [
    "# Part (b): Split the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iy51Q0txuJpC"
   },
   "outputs": [],
   "source": [
    "#====================================================#\n",
    "# YOUR CODE HERE:\n",
    "#  You should use the sklearn.model_selection.train_test_split\n",
    "#     parameter to perform the train/development split\n",
    "#   Set the test_size to 0.30.\n",
    "#   Set the random_stat parameter to 42.\n",
    "#====================================================#\n",
    "\n",
    "X_train_orig, X_develop_orig, y_train_orig, y_develop_orig = train_test_split(X_train, y_train, test_size = 0.3, random_state = 42)\n",
    "\n",
    "#====================================================#\n",
    "# END YOUR CODE\n",
    "#====================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSQOZqupuKGO"
   },
   "source": [
    "# Part (c): Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: By converting all words to lowercase, we have a more consistent format, and we will not need to worry about uppercase letter anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the words to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: When we lemmatize words, differemt variations of words (e.g., running, runs, and ran) are converted to their base form. This will simplify the analysis and increase the overall accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    # create a dict for mapping\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    # get the right tag\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    \n",
    "    # retrieve the tag for lemmatize\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Lemmatize all the words\n",
    "def lemmatization(text):\n",
    "    # Create a WordNetLemmatizer object    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Splitting text into individual words or tokens\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Apply lemmatization to each token in the list of words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos = get_pos(word)) for word in words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: Since punctuations in the sentence are not really meaningful in our analysis, eliminating all of them will make our analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip punctuation\n",
    "def punctuation_removal(text):\n",
    "    # create a table for translation\n",
    "    table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    \n",
    "    # translate the text using the table defined above\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason; Same as punctuations, the stopwords in the sentence are not meaningful and cannot provide any information. Therefore, eliminating them will reduce the complexity of the text and make our analysis simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the stop words, e.g.,“the”,“and”,“or”.\n",
    "def stopword_removal(text):\n",
    "    # list of stopwords in English\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # split the text into list\n",
    "    words = word_tokenize(text.strip())\n",
    "    \n",
    "    # filter out words that are not stopwords\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: URLs often contain different combinations of alphanumerics that do not contribute any meaningful information to the NLP tasks, eliminating them will help us reduce noise and focus on the text that are useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip urls.\n",
    "def url_removal(text):\n",
    "    return re.sub('https?://\\S+|www\\.\\S+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: Obviously, @ symbol does not have any meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip @\n",
    "def at_removal(text):\n",
    "    return text.replace('@','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: There are some strange characters (e.g., \"Ûªs\") that do not look like any kind of language. They are some kind of Mojibake that do not have any meaning. We need to eliminate them to ensure the text quality.Also, numbers are not really useful and thus we can eliminate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything else except alphabet\n",
    "def special(text):\n",
    "    return re.sub('[^a-zA-Z\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wWpMOdJ2uipq"
   },
   "outputs": [],
   "source": [
    "#=======================================================================+#\n",
    "# YOUR CODE HERE:\n",
    "#  You should complete the following function to obtain the pre-processed\n",
    "#  X_train and X_develop\n",
    "#  Note that we suggest you to do every sub-question in a dedicated Python\n",
    "#  function to make the code more structured and less error-prone.\n",
    "#  With a function, you can clearly test each part when you encounter an error.\n",
    "#  You can also create your own simple input data (e.g. just one sample) to\n",
    "#  test the correctness of a function.\n",
    "#========================================================================#\n",
    "def pre_process(data):\n",
    "    ''' the argument for this function is a dataframe'''\n",
    "    data.text = data.text.apply(lambda x : lowercase(x))\n",
    "    data.text = data.text.apply(lambda x : lemmatization(x))\n",
    "    data.text = data.text.apply(lambda x : punctuation_removal(x))\n",
    "    data.text = data.text.apply(lambda x : stopword_removal(x))\n",
    "    data.text = data.text.apply(lambda x : url_removal(x))\n",
    "    data.text = data.text.apply(lambda x : at_removal(x))\n",
    "    data.text = data.text.apply(lambda x : special(x))\n",
    "    \n",
    "    preproc_data = data\n",
    "    #========================================================================#\n",
    "    #  This function should return the pre-processed data\n",
    "    #========================================================================#\n",
    "    return preproc_data # Feel free to change the variable name\n",
    "\n",
    "# get the preprocessed data\n",
    "X_train_preproc   = pre_process(X_train_orig)\n",
    "X_develop_preproc = pre_process(X_develop_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GhiuEjdumEO"
   },
   "source": [
    "# Part (d): Bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to find the optimal threshold m, we need to iteratively test which value of m will yield the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best m for min_df is 1\n",
      "The highest score for this best_m is 0.7890804992556968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "m_lst = list(range(1,11))\n",
    "best_m = None\n",
    "highest_score = -1\n",
    "\n",
    "for i in m_lst:\n",
    "    count_vect = CountVectorizer(binary = True, min_df = i)\n",
    "    X_train = count_vect.fit_transform(X_train_preproc.text).toarray() # toarray() for better inspection\n",
    "    X_develop = count_vect.transform(X_develop_preproc.text).toarray()\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    scores = cross_val_score(model, X_train, y_train_orig, cv = 5)\n",
    "    \n",
    "    mean_score = scores.mean()\n",
    "    \n",
    "    if mean_score > highest_score:\n",
    "        highest_score = mean_score\n",
    "        best_m = i\n",
    "        \n",
    "print(f'The best m for min_df is {best_m}')\n",
    "print(f'The highest score for this best_m is {highest_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cross validation, the best value for \"min_df\" is 1, which will produce the highest score when we conduct Logistic Rregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fD4ZzrPQuo7B"
   },
   "outputs": [],
   "source": [
    "# There is no need to construct a function for this\n",
    "\n",
    "# vectorize the training set\n",
    "count_vect = CountVectorizer(binary = True, min_df = best_m)\n",
    "\n",
    "# transform the text into a count matrix\n",
    "X_train = count_vect.fit_transform(X_train_preproc.text).toarray() # toarray() for better inspection\n",
    "\n",
    "# Use the same count_vect object to transform the development set\n",
    "X_develop = count_vect.transform(X_develop_preproc.text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReEOC-rkupXV"
   },
   "source": [
    "# Part (e): Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic_without_regularization:\n",
    "> + The F1 score for training set is 0.9961, meaning the model fits the data pretty well.\n",
    "> + However, the F1 score for development set is 0.7288\n",
    "> + Therefore, there could be some overfitting in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vpjvSzoduu3O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for training set: 0.9961\n",
      "F1 for development set: 0.7288\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================+#\n",
    "# YOUR CODE HERE:\n",
    "#  You should complete the following function for logistic regression\n",
    "#  without regularization terms.\n",
    "#  You will be training logistic regression models using bag of words\n",
    "#  feature vectors obtained in part (d).\n",
    "#========================================================================#\n",
    "\n",
    "def logistic_without_regularization(X_train, Y_train, X_develop, Y_develop):\n",
    "    # initialize your logistic regression model\n",
    "    model = LogisticRegression(penalty = 'none', multi_class = 'ovr')\n",
    "    # then fit your model to the train data\n",
    "    model.fit(X_train, Y_train)\n",
    "    # then generate your prediction for the training set\n",
    "    y_train_no_reg = model.predict(X_train)\n",
    "\n",
    "    # then generate your prediction for the development set\n",
    "    y_develop_no_reg = model.predict(X_develop)\n",
    "    #========================================================================#\n",
    "    #  This function should train a logistic regression model without\n",
    "    #  regularization terms.\n",
    "    #  Report the F1 score in your training and in your development sets.\n",
    "    #========================================================================#\n",
    "    return y_train_no_reg, y_develop_no_reg\n",
    "\n",
    "# get the F1 train and develop scores\n",
    "F1_train_no_reg = f1_score(y_train_orig, logistic_without_regularization(X_train, y_train_orig, X_develop, y_develop_orig)[0])\n",
    "\n",
    "F1_develop_no_reg = f1_score(y_develop_orig, logistic_without_regularization(X_train, y_train_orig, X_develop, y_develop_orig)[1])\n",
    "\n",
    "# print the F1 train and develop scores\n",
    "print(f\"F1 for training set: {F1_train_no_reg:.4f}\")\n",
    "print(f\"F1 for development set: {F1_develop_no_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic_L1_regularization:\n",
    "> + The F1 score for training set is 0.8501, which is also good. Compared to 0.9961 in the logistic model without regularization, we introduce some bias in order to address overfitting.\n",
    "> + In logistic model with L1 regularization, the F1 score for development set is 0.7478, which is larger than 0.7288 (i.e., the F1 score of development set of logistic model without regularization.\n",
    "> + Since we slightly increase the performance of the model on the development set, this logistic model with L1 regularization performs better than the one without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eaZwAJP6KUfv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for training set: 0.8503\n",
      "F1 for development set: 0.7471\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================+#\n",
    "# YOUR CODE HERE:\n",
    "#  You should complete the following function for logistic regression\n",
    "#  with L1 regularization.\n",
    "#  You will be training logistic regression models using bag of words\n",
    "#  feature vectors obtained in part (d).\n",
    "#========================================================================#\n",
    "def logistic_L1_regularization(X_train, Y_train, X_develop, Y_develop):\n",
    "    # initialize your logistic regression model\n",
    "    model = LogisticRegression(penalty = 'l1', solver = 'liblinear', multi_class = 'ovr')\n",
    "    # then fit your model to the train data\n",
    "    model.fit(X_train, Y_train)\n",
    "    # then generate your prediction for the training set\n",
    "    y_train_L1_reg = model.predict(X_train)\n",
    "\n",
    "    # then generate your prediction for the development set\n",
    "    y_develop_L1_reg = model.predict(X_develop)\n",
    "    #========================================================================#\n",
    "    #  This function should train a logistic regression model with l1\n",
    "    #  regularization terms.\n",
    "    #  Report the F1 score in your training and in your development sets.\n",
    "    #========================================================================#\n",
    "    return y_train_L1_reg, y_develop_L1_reg\n",
    "\n",
    "# get the F1 train and develop scores\n",
    "F1_train_L1_reg = f1_score(y_train_orig, logistic_L1_regularization(X_train, y_train_orig, X_develop, y_develop_orig)[0])\n",
    "\n",
    "F1_develop_L1_reg = f1_score(y_develop_orig, logistic_L1_regularization(X_train, y_train_orig, X_develop, y_develop_orig)[1])\n",
    "\n",
    "# print the F1 train and develop scores\n",
    "print(f\"F1 for training set: {F1_train_L1_reg:.4f}\")\n",
    "print(f\"F1 for development set: {F1_develop_L1_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic_L2_regularization:\n",
    "> + The F1 score for training set is 0.9627, meaning the model fits the training data pretty well.\n",
    "> + In logistic model with L2 regularization, the F1 score for development set is 0.7533, which is larger than 0.7478 (i.e., the F1 score of development set of logistic model with L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9PHKy8ElKVAg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for training set: 0.9627\n",
      "F1 for development set: 0.7533\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================+#\n",
    "# YOUR CODE HERE:\n",
    "#  You should complete the following function for logistic regression\n",
    "#  with L2 regularization.\n",
    "#  You will be training logistic regression models using bag of words\n",
    "#  feature vectors obtained in part (d).\n",
    "#========================================================================#\n",
    "def logistic_L2_regularization(X_train, Y_train, X_develop, Y_develop):\n",
    "    # initialize your logistic regression model\n",
    "    model = LogisticRegression(penalty = 'l2', multi_class = 'ovr')\n",
    "    # then fit your model to the train data\n",
    "    model.fit(X_train, Y_train)\n",
    "    # then generate your prediction for the training set\n",
    "    y_train_L2_reg = model.predict(X_train)\n",
    "\n",
    "    # then generate your prediction for the development set\n",
    "    y_develop_L2_reg = model.predict(X_develop)\n",
    "    #========================================================================#\n",
    "    #  This function should train a logistic regression model with L2\n",
    "    #  regularization terms.\n",
    "    #  Report the F1 score in your training and in your development sets.\n",
    "    #========================================================================#\n",
    "    return y_train_L2_reg, y_develop_L2_reg\n",
    "\n",
    "# get the F1 train and develop scores\n",
    "F1_train_L2_reg = f1_score(y_train_orig, logistic_L2_regularization(X_train, y_train_orig, X_develop, y_develop_orig)[0])\n",
    "F1_develop_L2_reg = f1_score(y_develop_orig, logistic_L2_regularization(X_train, y_train_orig, X_develop, y_develop_orig)[1])\n",
    "\n",
    "# print the F1 train and develop scores\n",
    "print(f\"F1 for training set: {F1_train_L2_reg:.4f}\")\n",
    "print(f\"F1 for development set: {F1_develop_L2_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DmYQSKkK_L3"
   },
   "source": [
    "### Which one of the three classifiers performed the best on your training and development set? Did you observe any overfitting and did regularization help reduce it? Support your answers with the classifier performance you got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYpWFNElLbWv"
   },
   "source": [
    "### Answer:\n",
    "> + In general, the logistic regression model with L2 regularization performs best since it has the highest F1 scores for the development sets.\n",
    "> + In the logistic regression model without regularization, we notice that there exists overfitting since the F1 score for training set is much larger than the F1 score for the development set.\n",
    "> + By introducing some bias to the training set, both L1 and L2 regularization slightly reduce the overfitting since they have higher F1 score of the development set compared to the F1 score of development set of logistic regression model without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yysent_nLuh1"
   },
   "source": [
    "### Inspect the weight vector of the classifier with L1 regularization (in other words, look at the θ you got after training). You can access the weight vector of the trained model using the coef_attribute of a LogisticRegression instance. What are the most important words for deciding whether a tweet is about a real disaster or not? You might need to run some code (feel free to insert a code cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 spill\n",
      "2 derailment\n",
      "3 typhoon\n",
      "4 hiroshima\n",
      "5 earthquake\n",
      "6 migrant\n",
      "7 debris\n",
      "8 worth\n",
      "9 wildfire\n",
      "10 outbreak\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty = 'l1', solver = 'liblinear')\n",
    "model.fit(X_train, y_train_orig)\n",
    "\n",
    "coefficients = model.coef_[0]\n",
    "feature_importance = abs(coefficients)\n",
    "sorted_indices = feature_importance.argsort()[::-1]\n",
    "\n",
    "feature_name = count_vect.get_feature_names()\n",
    "\n",
    "top_words = 10\n",
    "for i,j in enumerate(range(top_words)):\n",
    "    print(i+1, feature_name[sorted_indices[j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ2S1QVCLwuY"
   },
   "source": [
    "### Answer: Most important words for deciding whether a tweet is about a real disaster or not (Top 10):\n",
    "> + 1 -> spill\n",
    "> + 2 -> derailment\n",
    "> + 3 -> typhoon\n",
    "> + 4 -> hiroshima\n",
    "> + 5 -> earthquake\n",
    "> + 6 -> migrant\n",
    "> + 7 -> debris\n",
    "> + 8 -> worth\n",
    "> + 9 -> wildfire\n",
    "> + 10 -> outbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UWCsGNruvXD"
   },
   "source": [
    "# Part (f): Bernoulli Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reference for this code is ChatGPT, however, I implemented it myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for training set: 0.9129\n",
      "F1 for development set: 0.7588\n"
     ]
    }
   ],
   "source": [
    "class BernoulliNB(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y): \n",
    "        # class k\n",
    "        self.K = np.unique(y)\n",
    "        \n",
    "        # create a dictionary to store conditional probabilities\n",
    "        self.conditional_probs = {}\n",
    "        self.feature_count = {}\n",
    "        \n",
    "        # create an array to store probs for each class\n",
    "        self.K_probs = np.array([])\n",
    "        \n",
    "        for k in self.K:\n",
    "            # filter each class\n",
    "            X_k = X[y == k]\n",
    "            \n",
    "            # calculate probs for each class\n",
    "            self.K_probs = np.append(self.K_probs, X_k.shape[0] / X.shape[0])\n",
    "            \n",
    "            # total number of features and feature counts for class k\n",
    "            total_features = X_k.sum()\n",
    "            feature_counts = X_k.sum(axis = 0)\n",
    "            \n",
    "            # Laplace smoothing\n",
    "            self.conditional_probs[k] = (feature_counts + self.alpha) / (total_features + self.alpha * X.shape[1])\n",
    "            self.feature_count[k] = total_features\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "\n",
    "        for i in X:\n",
    "            post = []\n",
    "            for k in self.K:\n",
    "                # log likelihood for k class\n",
    "                log_lh = np.sum(np.log(self.conditional_probs[k]) * i)\n",
    "\n",
    "                # log posterior for k class\n",
    "                log_post = log_lh + np.log(self.K_probs[k])\n",
    "\n",
    "                post.append(log_post)\n",
    "\n",
    "            # set the class with the highest log posterior\n",
    "            y_pred.append(self.K[np.argmax(post)])\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "# get the predictions y_train_NB and y_develop_NB\n",
    "nb = BernoulliNB(alpha=1)\n",
    "nb.fit(X_train, y_train_orig)\n",
    "y_train_NB = nb.predict(X_train) # prediction from X_train using model\n",
    "y_develop_NB = nb.predict(X_develop) # prediction from X_develop using model\n",
    "\n",
    "# get the F1 train and develop scores\n",
    "F1_train_NB = f1_score(y_train_orig, y_train_NB)\n",
    "F1_develop_NB = f1_score(y_develop_orig, y_develop_NB)\n",
    "\n",
    "# print the F1 train and develop scores\n",
    "print(f\"F1 for training set: {F1_train_NB:.4f}\")\n",
    "print(f\"F1 for development set: {F1_develop_NB:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JQrNzE6u0jY"
   },
   "source": [
    "# Part (g): Model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7BcX1e7Ti0J"
   },
   "source": [
    "Question: Which model performed the best in predicting whether a tweet is of a real disaster or not? Include your performance metric in your response. Comment on the pros and cons of using generative vs discriminative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7NDJXWFTn4m"
   },
   "source": [
    "Answer: \n",
    "#### Sources: ChatGPT and Classnotes\n",
    "> + In predicting whether a tweet is of a real disaster or not, the generative classifier (i.e., Bernoulli Naive Bayes) performs best since it has the highest F1 score for the development set, which is 0.7588.\n",
    "> + **Generative models:**\n",
    "> + Pros:\n",
    ">> 1. Generative models require less sample. They perform well with fewer labeled exmaples since they can capture the underlying data distribution.\n",
    ">> 2. Generative models can handle missing data better since thye model the joint distribution. They can infer missing features and generate samples with missing values.\n",
    "> + Cons:\n",
    ">> 1. Compared to discriminative models, generative models are generally more complex and computationally expensive.\n",
    ">> 2. If training data is limited, generative models are prone to overfitting since they capture the full data distribution.\n",
    "> + **Discriminative models:**\n",
    "> + Pros:\n",
    ">> 1. In practice, discriminative models are often more accurate for classification tasks.\n",
    ">> 2. Discriminative models are simpler to train since they focus on modeling the decision boundary between classes rather than the whole data distribution.\n",
    "\n",
    "> + Cons:\n",
    ">> 1. The performance of discriminative models can be affected by out-of-distribution data because they do not model the uncertainty in the data distribution as explicitly as generative models.\n",
    ">> 2. Unlike generative models, discriminative models need a larger amount of labeled data to perform well, especially when class boundaries are not well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTrNTtLgTpFM"
   },
   "source": [
    "Question: hink about the assumptions that Naive Bayes makes. How are the assumptions different from logistic regressions? Discuss whether it is valid and efficient to use Bernoulli Naive Bayes classifier for natural language texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvS8MuqBTqIR"
   },
   "source": [
    "Answer:\n",
    "#### Sources: ChatGPT\n",
    "#### Differences in assumptions:\n",
    "> + Logistic regression does not assume independence amoung features. However, Naive Bayes assumes feature independence.\n",
    "> + Logistic regression assumes a linear relationship between the log-odds of features and the target variable. However,Naive Bayes does not make this assumption, and instead, it models the probability distribution of features given the class.\n",
    "> + Logistic regression model is sensitive to outliers. However, Naive Bayes is less sensitive to outliers since it estimates probabilities based on feature counts.\n",
    "\n",
    "#### Use Bernoulli Naive Bayes classifier for natural language texts\n",
    "> + Bernoulli Naive Bayes is suited for binary text classifications. When we try to calssify documents into two classes such as spam vs. non spam, Bernoulli Naive Bayes treats each feature as binary, indicating whether a word is present (1) or not present (0) in the document.\n",
    "> + In text classification, the vocabulary can be very large, and Bernoulli Naive Bayes handles high-dimentional data efficiently.\n",
    "> + For relatively large-scale text classification tasks, Bernoulli Naive Bayes is well-suited since it is computationally efficient.\n",
    "> + Bernoulli Naive Bayes are less sensitive to noisy words since it only focus on whether words are present or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46y89mf2u4q2"
   },
   "source": [
    "# Part (h): N-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar to what we have done in the bag of words section, we need to iteratively test which value of m will yield the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best m for min_df is 2\n",
      "The highest score for this best_m is 0.7361597477296551\n"
     ]
    }
   ],
   "source": [
    "m_lst = list(range(1,11))\n",
    "best_m = None\n",
    "highest_score = -1\n",
    "\n",
    "for i in m_lst:\n",
    "    count_vect = CountVectorizer(binary = True, min_df = i, ngram_range = (2,2))\n",
    "    X_train_gram = count_vect.fit_transform(X_train_preproc.text).toarray() # toarray() for better inspection\n",
    "    X_develop_gram = count_vect.transform(X_develop_preproc.text).toarray()\n",
    "    \n",
    "    # use logistic regression with L2 regularization since it performs the best\n",
    "    model = LogisticRegression(penalty = 'l2', multi_class = 'ovr')\n",
    "    scores = cross_val_score(model, X_train_gram, y_train_orig, cv = 5)\n",
    "    \n",
    "    mean_score = scores.mean()\n",
    "    \n",
    "    if mean_score > highest_score:\n",
    "        highest_score = mean_score\n",
    "        best_m = i\n",
    "        \n",
    "print(f'The best m for min_df is {best_m}')\n",
    "print(f'The highest score for this best_m is {highest_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cross validation, the best value for \"min_df\" is 2, which will produce the highest score when we conduct Logistic Rregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the training set\n",
    "count_vect = CountVectorizer(binary = True, min_df = best_m, ngram_range = (2,2))\n",
    "\n",
    "# transform the text into a count matrix\n",
    "X_train_gram = count_vect.fit_transform(X_train_preproc.text).toarray() # toarray() for better inspection\n",
    "\n",
    "# Use the same count_vect object to transform the development set\n",
    "X_develop_gram = count_vect.transform(X_develop_preproc.text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the total number of 2-grams in your vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of 2-grams in the vocabulary is: 4055\n"
     ]
    }
   ],
   "source": [
    "print(f'The total number of 2-grams in the vocabulary is: {X_train_gram.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In addition, take 10 2-grams fromyour vocabulary, and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are 10 randomly selected 2-grams from the vocabulary: \n",
      "1 militant suicide\n",
      "2 like mudslide\n",
      "3 centipede press\n",
      "4 brian ruebs\n",
      "5 youtube playlist\n",
      "6 mod showcase\n",
      "7 would work\n",
      "8 content http\n",
      "9 avalanche http\n",
      "10 sit right\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "grams = np.random.choice(list(count_vect.vocabulary_.keys()), size = 10, replace = False)\n",
    "\n",
    "print('Below are 10 randomly selected 2-grams from the vocabulary: ')\n",
    "for i,j in enumerate(grams):\n",
    "    print(i+1, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement logistic regression models and the Bernoulli Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "F7q6X9Geu8NB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for training set: 0.83\n",
      "F1 for development set: 0.63\n",
      "--------------------------------------------------------------------------------\n",
      "F1 for training set: 0.70\n",
      "F1 for development set: 0.56\n",
      "--------------------------------------------------------------------------------\n",
      "F1 for training set: 0.76\n",
      "F1 for development set: 0.62\n",
      "--------------------------------------------------------------------------------\n",
      "F1 for training set: 0.75\n",
      "F1 for development set: 0.62\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================+#\n",
    "# YOUR CODE HERE:\n",
    "#  Use the functions you already defined \"X_train_gram\" and \"X_develop_gram\"\n",
    "#  to re-run:\n",
    "#  Logistic Regression with no regularization Model\n",
    "#  Logistic Regression with L1 regularization Model\n",
    "#  Logistic Regression with L2 regularization Model\n",
    "#========================================================================#\n",
    "# Logistic Regression with no regularization Model\n",
    "y_train_gram_no_reg, y_develop_gram_no_reg = logistic_without_regularization(X_train_gram, y_train_orig, X_develop_gram, y_develop_orig)\n",
    "\n",
    "# Logistic Regression with L1 regularization Model\n",
    "y_train_gram_L1_reg, y_develop_gram_L1_reg = logistic_L1_regularization(X_train_gram, y_train_orig, X_develop_gram, y_develop_orig)\n",
    "\n",
    "#  Logistic Regression with L2 regularization Model\n",
    "y_train_gram_L2_reg, y_develop_gram_L2_reg = logistic_L2_regularization(X_train_gram, y_train_orig, X_develop_gram, y_develop_orig)\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "nb = BernoulliNB(alpha=1)\n",
    "nb.fit(X_train_gram, y_train_orig)\n",
    "\n",
    "y_train_gram_NB = nb.predict(X_train_gram)\n",
    "y_develop_gram_NB = nb.predict(X_develop_gram)\n",
    "#========================================================================#\n",
    "# END CODE HERE\n",
    "#========================================================================#\n",
    "\n",
    "# get the F1 train and develop scores for no regularization model\n",
    "F1_train_gram_no_reg = f1_score(y_train_orig, y_train_gram_no_reg)\n",
    "F1_develop_gram_no_reg = f1_score(y_develop_orig, y_develop_gram_no_reg)\n",
    "\n",
    "# get the F1 train and develop scores for L1 regularization model\n",
    "F1_train_gram_L1_reg = f1_score(y_train_orig, y_train_gram_L1_reg)\n",
    "F1_develop_gram_L1_reg = f1_score(y_develop_orig, y_develop_gram_L1_reg)\n",
    "\n",
    "# get the F1 train and develop scores for L2 regularization model\n",
    "F1_train_gram_L2_reg = f1_score(y_train_orig, y_train_gram_L2_reg)\n",
    "F1_develop_gram_L2_reg = f1_score(y_develop_orig, y_develop_gram_L2_reg)\n",
    "\n",
    "# get the F1 train and develop scores for Bernoulli NB model\n",
    "F1_train_gram_NB = f1_score(y_train_orig, y_train_gram_NB)\n",
    "F1_develop_gram_NB = f1_score(y_develop_orig, y_develop_gram_NB)\n",
    "\n",
    "# print the F1 train and develop scores for no regularization model\n",
    "print(f\"F1 for training set: {F1_train_gram_no_reg:.2f}\")\n",
    "print(f\"F1 for development set: {F1_develop_gram_no_reg:.2f}\")\n",
    "\n",
    "print('-' * 80)\n",
    "\n",
    "# print the F1 train and develop scores for L1 regularization model\n",
    "print(f\"F1 for training set: {F1_train_gram_L1_reg:.2f}\")\n",
    "print(f\"F1 for development set: {F1_develop_gram_L1_reg:.2f}\")\n",
    "\n",
    "print('-' * 80)\n",
    "\n",
    "# print the F1 train and develop scores for L2 regularization model\n",
    "print(f\"F1 for training set: {F1_train_gram_L2_reg:.2f}\")\n",
    "print(f\"F1 for development set: {F1_develop_gram_L2_reg:.2f}\")\n",
    "\n",
    "print('-' * 80)\n",
    "\n",
    "# print the F1 train and develop scores for Bernoulli NB model\n",
    "print(f\"F1 for training set: {F1_train_gram_NB:.2f}\")\n",
    "print(f\"F1 for development set: {F1_develop_gram_NB:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "> + From the above results, we can see that no matter what model we select, the F1 scores are all really low when we use 2-gram model. However, the F1 scores we get from bag of words model are much better than these results.\n",
    "> + This implies that, in this problem, bag of words model is a better text representation technique and can produce better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55QmSb3ku8oa"
   },
   "source": [
    "# Part (i): Determine performance with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "i270_AqVvAbH"
   },
   "outputs": [],
   "source": [
    "# we need to concatenate both train and development set\n",
    "# Also, we choose the bag of word model\n",
    "all_data_X = np.concatenate((X_train, X_develop), axis = 0)\n",
    "\n",
    "all_data_y = np.concatenate((y_train_orig, y_develop_orig), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to save the id column of the test data for submission\n",
    "index_X_test = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we train the model, need to preprocess the test data as well\n",
    "X_test_preproc = pre_process(X_test)\n",
    "\n",
    "# apply the bag of word model on test data\n",
    "count_vect = CountVectorizer(binary = True, min_df = 1)\n",
    "\n",
    "# transform the text into a count matrix\n",
    "X_train = count_vect.fit_transform(X_train_preproc.text).toarray() # toarray() for better inspection\n",
    "\n",
    "# Use the same count_vect object to transform the test set\n",
    "X_test = count_vect.transform(X_test_preproc.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "u985um3AY1ie"
   },
   "outputs": [],
   "source": [
    "# since the Bernoulli Naive Bayes classifier performs best in this problem, we will use it\n",
    "nb = BernoulliNB(alpha=1)\n",
    "nb.fit(all_data_X, all_data_y)\n",
    "\n",
    "y_prediction = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = index_X_test\n",
    "submission['target'] = y_prediction\n",
    "\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the resulting F1-score on the test data, as reported by Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "6CIKyutPY1zL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score on Kaggle is: 0.78731\n"
     ]
    }
   ],
   "source": [
    "print(f'The score on Kaggle is: {0.78731}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was this lower or higher than you expected? Discuss why it might be lower or higher than your expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score (i.e.,0.78731) on the test data is higher than my expectation since the F1 score on the developement set using bag of words model and Bernoulli Naive Bayes is 0.7588. One reason why the result is higher than my expectation could be due to the randomness when splitting the data. Since we set random_state = 42 when running the sklearn function \"train_test_split\", the train data and development data are fixed every time I run the function. However, if I do not set the random_state, each time I will get a different train data and development data, thus will affect the model performance and produce different F1 score. In this case, I could get a higher, lower, or approximately the same F1 score compared to the one I get from Kaggle, therefore, it is reasonable that the result I get from Kaggle is higher than my expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
